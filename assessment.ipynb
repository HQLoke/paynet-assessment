{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Analysis Assessment\n",
    "\n",
    "##### Create a pyspark session\n",
    "##### Read from json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"JSON Reader\").getOrCreate()\n",
    "\n",
    "df = spark.read.json(\"data/cc_sample_transaction.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3a Parsing personal details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json, split\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType\n",
    "\n",
    "personal_detail_schema = StructType([\n",
    "    StructField(\"person_name\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"lat\", StringType(), True),\n",
    "    StructField(\"long\", StringType(), True),\n",
    "    StructField(\"city_pop\", StringType(), True),\n",
    "    StructField(\"job\", StringType(), True),\n",
    "    StructField(\"dob\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name derivation\n",
    "# Parse personal_detail as Struct\n",
    "# Split person_name into first and last\n",
    "# ! Need to split this to explain further\n",
    "# Initially thought that each person's name is split by an empty space, further analysis shows it could be any delimiter\n",
    "# Split based on regex\n",
    "# For example, some of the names were added with NOOOO in the dirty data, using this alphabetical regex split and only getting the first and second items seem to solve this\n",
    "# The rest is pretty straight-forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"personal_detail\", from_json(\"personal_detail\", personal_detail_schema))\n",
    "\n",
    "df = df.withColumn(\"first\", split(col(\"personal_detail.person_name\"), \"[^A-Za-z]+\").getItem(0))\n",
    "df = df.withColumn(\"last\", split(col(\"personal_detail.person_name\"), \"[^A-Za-z]+\").getItem(1))\n",
    "\n",
    "df = df.withColumn(\"gender\", col(\"personal_detail.gender\"))\n",
    "df = df.withColumn(\"city_pop\", col(\"personal_detail.city_pop\"))\n",
    "df = df.withColumn(\"job\", col(\"personal_detail.job\"))\n",
    "df = df.withColumn(\"dob\", col(\"personal_detail.dob\"))\n",
    "df = df.withColumn(\"lat\", col(\"personal_detail.lat\"))\n",
    "df = df.withColumn(\"long\", col(\"personal_detail.long\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3b Parse address into street, city, state, zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_schema = StructType([\n",
    "    StructField(\"street\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"zip\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the rest of the details from address struct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"address\", from_json(col(\"personal_detail.address\"), address_schema))\n",
    "\n",
    "df = df.withColumn(\"street\", col(\"address.street\"))\n",
    "df = df.withColumn(\"city\", col(\"address.city\"))\n",
    "df = df.withColumn(\"state\", col(\"address.state\"))\n",
    "df = df.withColumn(\"zip\", col(\"address.zip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3c Convert timestamps to human-readable format in UTC+8\n",
    "\n",
    "Because I'm given two types of epoch time, 13 and 16 digits length, which are milli and microseconds respectively.\n",
    "So use an if statement to check how long before dividing.\n",
    "\n",
    "Previously I thought that the epoch time given is utc time, so I used the from_utc_timestamp to convert, but it's actually local time utc+8, so I can skip doing another conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_utc_timestamp, from_unixtime, col, date_format, expr\n",
    "\n",
    "df = df.withColumn(\"trans_date_trans_time\", date_format(from_utc_timestamp(col(\"trans_date_trans_time\"), \"Asia/Singapore\"), \"yyyy-MM-dd HH:mm\"))\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"merch_last_update_time\",\n",
    "    date_format(\n",
    "        from_unixtime(expr(\"merch_last_update_time / (IF(LENGTH(merch_last_update_time) = 16, 1000000, 1000))\")),\n",
    "        \"yyyy-MM-dd HH:mm\"\n",
    "    )\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"merch_eff_time\",\n",
    "    date_format(\n",
    "        from_unixtime(expr(\"merch_eff_time / (IF(LENGTH(merch_eff_time) = 16, 1000000, 1000))\")),\n",
    "        \"yyyy-MM-dd HH:mm\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3d Drop unnecessary columns\n",
    "Since there are only used as a placeholder for struct data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"personal_detail\", \"address\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Check for data inconsistencies\n",
    "\n",
    "##### Check missing or null values\n",
    "##### Check inconsistent format\n",
    "##### Check categorical value\n",
    "##### Check duplicate row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+--------+------+------+--------+--------------+----------------------+---------+----------+-------------+--------+---------------------+---------+-----+----+------+--------+---+---+---+----+------+----+-----+---+\n",
      "|Unnamed: 0|amt|category|cc_bic|cc_num|is_fraud|merch_eff_time|merch_last_update_time|merch_lat|merch_long|merch_zipcode|merchant|trans_date_trans_time|trans_num|first|last|gender|city_pop|job|dob|lat|long|street|city|state|zip|\n",
      "+----------+---+--------+------+------+--------+--------------+----------------------+---------+----------+-------------+--------+---------------------+---------+-----+----+------+--------+---+---+---+----+------+----+-----+---+\n",
      "+----------+---+--------+------+------+--------+--------------+----------------------+---------+----------+-------------+--------+---------------------+---------+-----+----+------+--------+---+---+---+----+------+----+-----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 193:>                                                      (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 0 rows with empty values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, upper\n",
    "\n",
    "# Get all column names\n",
    "columns_to_check = [\"first\", \"last\", \"cc_num\", \"amt\", \"trans_date_trans_time\", \"merch_last_update_time\", \"merch_eff_time\"]\n",
    "\n",
    "# Build the filter condition dynamically for null, empty, \"N/A\", \"NA\" and \"None\"\n",
    "condition = None\n",
    "for c in columns_to_check:\n",
    "    col_condition = (\n",
    "        col(c).isNull() |  # Check for null values\n",
    "        (col(c) == \"\") |  # Check for empty strings\n",
    "        (upper(col(c)) == \"N/A\") |  # Check for \"N/A\" (case-insensitive)\n",
    "        (upper(col(c)) == \"NA\") |  # Check for \"NA\" (case-insensitive)\n",
    "        (upper(col(c)) == \"NONE\")  # Check for \"None\" (case-insensitive)\n",
    "    )\n",
    "    condition = col_condition if condition is None else condition | col_condition\n",
    "\n",
    "# Apply the filter to find dirty rows\n",
    "dirty_rows = df.filter(condition)\n",
    "\n",
    "# Show the dirty rows (or you can save it, analyze, etc.)\n",
    "dirty_rows.show(truncate=False)\n",
    "print(f\"There is {dirty_rows.count()} rows with empty values.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+--------+------+------+--------+--------------+----------------------+---------+----------+-------------+--------+---------------------+---------+-----+----+------+--------+---+---+---+----+------+----+-----+---+\n",
      "|Unnamed: 0|amt|category|cc_bic|cc_num|is_fraud|merch_eff_time|merch_last_update_time|merch_lat|merch_long|merch_zipcode|merchant|trans_date_trans_time|trans_num|first|last|gender|city_pop|job|dob|lat|long|street|city|state|zip|\n",
      "+----------+---+--------+------+------+--------+--------------+----------------------+---------+----------+-------------+--------+---------------------+---------+-----+----+------+--------+---+---+---+----+------+----+-----+---+\n",
      "+----------+---+--------+------+------+--------+--------------+----------------------+---------+----------+-------------+--------+---------------------+---------+-----+----+------+--------+---+---+---+----+------+----+-----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 199:>                                                      (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 0 invalid time rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "time_columns = [\"trans_date_trans_time\", \"merch_last_update_time\", \"merch_eff_time\"]\n",
    "\n",
    "condition = None\n",
    "\n",
    "# Regex pattern for yyyy-MM-dd HH:mm\n",
    "time_format_regex = r'^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}$'\n",
    "\n",
    "# Loop through each time column\n",
    "for c in time_columns:\n",
    "    col_condition = ~col(c).rlike(time_format_regex)  # Check for invalid format\n",
    "    condition = col_condition if condition is None else condition | col_condition\n",
    "    \n",
    "invalid_time_rows = df.filter(condition)\n",
    "invalid_time_rows.show(truncate=False)\n",
    "\n",
    "print(f\"There is {invalid_time_rows.count()} invalid time rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+--------+------+------+--------+--------------+----------------------+---------+----------+-------------+--------+---------------------+---------+-----+----+------+--------+---+---+---+----+------+----+-----+---+\n",
      "|Unnamed: 0|amt|category|cc_bic|cc_num|is_fraud|merch_eff_time|merch_last_update_time|merch_lat|merch_long|merch_zipcode|merchant|trans_date_trans_time|trans_num|first|last|gender|city_pop|job|dob|lat|long|street|city|state|zip|\n",
      "+----------+---+--------+------+------+--------+--------------+----------------------+---------+----------+-------------+--------+---------------------+---------+-----+----+------+--------+---+---+---+----+------+----+-----+---+\n",
      "+----------+---+--------+------+------+--------+--------------+----------------------+---------+----------+-------------+--------+---------------------+---------+-----+----+------+--------+---+---+---+----+------+----+-----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 215:>                                                      (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 0 rows with invalid gender.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "invalid_gender_rows = df.filter(~col(\"gender\").isin([\"M\", \"F\"]))\n",
    "\n",
    "# Show the result\n",
    "invalid_gender_rows.show(truncate=False)\n",
    "print(f\"There is {invalid_gender_rows.count()} rows with invalid gender.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
