{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Analysis Assessment\n",
    "\n",
    "##### Create a pyspark session\n",
    "##### Read from json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"JSON Reader\").getOrCreate()\n",
    "\n",
    "df = spark.read.json(\"data/cc_sample_transaction.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3a Parsing personal details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json, split\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType\n",
    "\n",
    "personal_detail_schema = StructType([\n",
    "    StructField(\"person_name\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"lat\", StringType(), True),\n",
    "    StructField(\"long\", StringType(), True),\n",
    "    StructField(\"city_pop\", StringType(), True),\n",
    "    StructField(\"job\", StringType(), True),\n",
    "    StructField(\"dob\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name derivation\n",
    "# Parse personal_detail as Struct\n",
    "# Split person_name into first and last\n",
    "# ! Need to split this to explain further\n",
    "# Initially thought that each person's name is split by an empty space, further analysis shows it could be any delimiter\n",
    "# Split based on regex\n",
    "# For example, some of the names were added with NOOOO in the dirty data, using this alphabetical regex split and only getting the first and second items seem to solve this\n",
    "# The rest is pretty straight-forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"personal_detail\", from_json(\"personal_detail\", personal_detail_schema))\n",
    "\n",
    "df = df.withColumn(\"first\", split(col(\"personal_detail.person_name\"), \"[^A-Za-z]+\").getItem(0))\n",
    "df = df.withColumn(\"last\", split(col(\"personal_detail.person_name\"), \"[^A-Za-z]+\").getItem(1))\n",
    "\n",
    "df = df.withColumn(\"gender\", col(\"personal_detail.gender\"))\n",
    "df = df.withColumn(\"city_pop\", col(\"personal_detail.city_pop\"))\n",
    "df = df.withColumn(\"job\", col(\"personal_detail.job\"))\n",
    "df = df.withColumn(\"dob\", col(\"personal_detail.dob\"))\n",
    "df = df.withColumn(\"lat\", col(\"personal_detail.lat\"))\n",
    "df = df.withColumn(\"long\", col(\"personal_detail.long\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3b Parse address into street, city, state, zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_schema = StructType([\n",
    "    StructField(\"street\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"zip\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the rest of the details from address struct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"address\", from_json(col(\"personal_detail.address\"), address_schema))\n",
    "\n",
    "df = df.withColumn(\"street\", col(\"address.street\"))\n",
    "df = df.withColumn(\"city\", col(\"address.city\"))\n",
    "df = df.withColumn(\"state\", col(\"address.state\"))\n",
    "df = df.withColumn(\"zip\", col(\"address.zip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3c Convert timestamps to human-readable format in UTC+8\n",
    "\n",
    "Because I'm given two types of epoch time, 13 and 16 digits length, which are milli and microseconds respectively.\n",
    "So use an if statement to check how long before dividing.\n",
    "\n",
    "Previously I thought that the epoch time given is utc time, so I used the from_utc_timestamp to convert, but it's actually local time utc+8, so I can skip doing another conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_utc_timestamp, from_unixtime, col, date_format, expr\n",
    "\n",
    "df = df.withColumn(\"trans_date_trans_time\", date_format(from_utc_timestamp(col(\"trans_date_trans_time\"), \"Asia/Singapore\"), \"yyyy-MM-dd HH:mm\"))\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"merch_last_update_time\",\n",
    "    date_format(\n",
    "        from_unixtime(expr(\"merch_last_update_time / (IF(LENGTH(merch_last_update_time) = 16, 1000000, 1000))\")),\n",
    "        \"yyyy-MM-dd HH:mm\"\n",
    "    )\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"merch_eff_time\",\n",
    "    date_format(\n",
    "        from_unixtime(expr(\"merch_eff_time / (IF(LENGTH(merch_eff_time) = 16, 1000000, 1000))\")),\n",
    "        \"yyyy-MM-dd HH:mm\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3d Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------------+-----------+----------------+--------+----------------+----------------------+------------------+------------------+-------------+--------------------+---------------------+--------------------+---------+--------+------+--------+--------------------+----------+-------+------------------+--------------------+--------------+-----+-----+\n",
      "|Unnamed: 0|   amt|     category|     cc_bic|          cc_num|is_fraud|  merch_eff_time|merch_last_update_time|         merch_lat|        merch_long|merch_zipcode|            merchant|trans_date_trans_time|           trans_num|    first|    last|gender|city_pop|                 job|       dob|    lat|              long|              street|          city|state|  zip|\n",
      "+----------+------+-------------+-----------+----------------+--------+----------------+----------------------+------------------+------------------+-------------+--------------------+---------------------+--------------------+---------+--------+------+--------+--------------------+----------+-------+------------------+--------------------+--------------+-----+-----+\n",
      "|         0|  4.97|     misc_net|CITIUS33CHI|2703186189652095|       0|2012-01-01 08:00|      2012-01-01 08:00|         36.011293|        -82.048315|        28705|fraud_Rippin, Kub...|     2019-01-01 08:00|0b242abb623afc578...| Jennifer|   Banks|     F|    3495|Psychologist, cou...|1988-03-09|36.0788|          -81.1781|      561 Perry Cove|Moravian Falls|   NC|28654|\n",
      "|         1|107.23|  grocery_pos|   ADMDUS41|    630423337322|       0|2012-01-01 08:00|      1974-03-15 07:30|49.159046999999994|       -118.186462|         NULL|fraud_Heller, Gut...|     2019-01-01 08:00|1f76529f857473494...|Stephanie|    Gill|     F|     149|Special education...|1978-06-21|48.8878|         -118.2105|43039 Riley Green...|        Orient|   WA|99160|\n",
      "|         2|220.11|entertainment|       Null|  38859492057661|       0|2012-01-01 08:00|      2012-01-01 08:00|         43.150704|       -112.154481|        83236|fraud_Lind-Buckridge|     2019-01-01 08:00|a1a22d70485983eac...|   Edward| Sanchez|     M|    4154|Nature conservati...|1962-01-19|42.1808|          -112.262|594 White Dale Su...|    Malad City|   ID|83252|\n",
      "|         3|  45.0|gas_transport|DEUTUS33TRF|3534093764340240|       0|2012-01-01 08:01|      2012-01-01 08:01|         47.034331|       -112.561071|         NULL|fraud_Kutch, Herm...|     2019-01-01 08:01|6b849c168bdad6f86...|   Jeremy|   White|     M|    1939|     Patent attorney|1967-01-12|46.2306|         -112.1138|9443 Cynthia Cour...|       Boulder|   MT|59632|\n",
      "|         4| 41.96|     misc_pos|   APBCUS61| 375534208663984|       0|2012-01-01 08:03|      1974-03-15 07:30|         38.674999|        -78.632459|        22844| fraud_Keeling-Crist|     2019-01-01 08:03|a41d7549acf907893...|    Tyler|  Garcia|     M|      99|Dance movement ps...|1986-03-28|38.4207|          -79.4629|    408 Bradley Rest|      Doe Hill|   VA|24433|\n",
      "|         5| 94.63|gas_transport|   APBCUS61|4767265376804500|       0|2012-01-01 08:04|      2012-01-01 08:04|         40.653382|-76.15266700000001|        17972|fraud_Stroman, Hu...|     2019-01-01 08:04|189a841a0a8ba0305...| Jennifer|  Conner|     F|    2158|   Transport planner|1961-06-19| 40.375|          -75.2045|   4655 David Island|        Dublin|   PA|18917|\n",
      "|         6| 44.54|  grocery_net|   APBCUS61|  30074693890476|       0|2012-01-01 08:04|      2012-01-01 08:04|37.162704999999995|        -100.15337|         NULL|fraud_Rowe-Vander...|     2019-01-01 08:04|83ec1cc84142af6e2...|   Kelsey|Richards|     F|    2691|     Arboriculturist|1993-08-16|37.9931|         -100.9893|889 Sarah Station...|       Holcomb|   KS|67851|\n",
      "|         7| 71.65|gas_transport|         NA|6011360759745864|       0|2012-01-01 08:05|      2012-01-01 08:05|         38.948089|        -78.540296|        22644|fraud_Corwin-Collins|     2019-01-01 08:05|6d294ed2cc447d2c7...|   Steven|Williams|     M|    6018|Designer, multimedia|1947-08-21|38.8432|          -78.6003|231 Flores Pass S...|      Edinburg|   VA|22824|\n",
      "|         8|  4.27|     misc_pos|   ACEEUS31|4922710831011201|       0|2012-01-01 08:05|      2012-01-01 08:05|         40.351813|        -79.958146|        15236|    fraud_Herzog Ltd|     2019-01-01 08:05|fc28024ce480f8ef2...|  Heather|   Chase|     F|    1472|Public affairs co...|1941-03-07|40.3359|          -79.6607|6888 Hicks Stream...|         Manor|   PA|15665|\n",
      "|         9|198.39|  grocery_pos|DEUTUS33TRF|2720830304681674|       0|2012-01-01 08:06|      2012-01-01 08:06|         37.179198|        -87.485381|        42442|fraud_Schoen, Kup...|     2019-01-01 08:06|3b9014ea8fb80bd65...|  Melissa| Aguilar|     F|  151785|         Pathologist|1974-03-28| 36.522|-87.34899999999999|21326 Taylor Squa...|   Clarksville|   TN|37040|\n",
      "+----------+------+-------------+-----------+----------------+--------+----------------+----------------------+------------------+------------------+-------------+--------------------+---------------------+--------------------+---------+--------+------+--------+--------------------+----------+-------+------------------+--------------------+--------------+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1296675"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(\"personal_detail\", \"address\")\n",
    "\n",
    "df.show(10)\n",
    "df.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
